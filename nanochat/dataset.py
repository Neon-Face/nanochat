"""
IPv6 Dataset Loader for 6GPT.
Reads local Parquet files generated by `dev/prepare_ipv6_parquet.py`.

Directory: ~/.cache/nanochat/base_data
"""

import os
import pyarrow.parquet as pq
from nanochat.common import get_base_dir

# -----------------------------------------------------------------------------
# Configuration

# 获取 NanoChat 的基础目录 (通常是 ~/.cache/nanochat)
base_dir = get_base_dir()
# 数据存放的子目录
DATA_DIR = os.path.join(base_dir, "base_data")

# -----------------------------------------------------------------------------
# Utilities

def list_parquet_files(data_dir=None):
    """
    扫描 DATA_DIR，按文件名排序返回所有 .parquet 文件的完整路径。
    """
    data_dir = DATA_DIR if data_dir is None else data_dir
    
    if not os.path.exists(data_dir):
        print(f"ERROR: Data directory not found at: {data_dir}")
        print("Did you run 'python dev/prepare_ipv6_parquet.py'?")
        return []

    # 过滤出 .parquet 文件并排序
    parquet_files = sorted([
        f for f in os.listdir(data_dir)
        if f.endswith('.parquet') and not f.endswith('.tmp')
    ])
    
    if not parquet_files:
        print(f"WARNING: No .parquet files found in {data_dir}")
        return []

    # 拼接完整路径
    parquet_paths = [os.path.join(data_dir, f) for f in parquet_files]
    return parquet_paths

def parquets_iter_batched(split, start=0, step=1):
    """
    核心生成器：流式读取 Parquet 文件，提取 'text' 列的内容。
    
    Args:
        split (str): 'train' 或 'val'
        start (int): 用于分布式训练 (DDP)，当前进程的 rank
        step (int): 用于分布式训练 (DDP)，总进程数 world_size
    """
    assert split in ["train", "val"], "split must be 'train' or 'val'"
    
    parquet_paths = list_parquet_files()
    
    if not parquet_paths:
        return

    # 简单的切分逻辑：
    # val   = 最后一个文件
    # train = 除了最后一个之外的所有文件
    if len(parquet_paths) == 1:
        # 如果只有一个文件，train 和 val 都用它（避免报错），但会警告
        files_to_read = parquet_paths
        if split == 'val':
            print("WARNING: Only 1 shard found. Using it for validation too (data leakage).")
    else:
        if split == "train":
            files_to_read = parquet_paths[:-1] 
        else:
            files_to_read = parquet_paths[-1:] # 只取最后一个做验证

    # 开始遍历文件
    for filepath in files_to_read:
        try:
            pf = pq.ParquetFile(filepath)
            # 按 Row Group 读取 (防止一次性把整个文件读入内存)
            # step 参数用于多 GPU 训练时，让不同 GPU 读不同的 Row Group，互不干扰
            for rg_idx in range(start, pf.num_row_groups, step):
                rg = pf.read_row_group(rg_idx)
                # 'text' 列是在 prepare_ipv6_parquet.py 里定义的
                if 'text' in rg.column_names:
                    texts = rg.column('text').to_pylist()
                    yield texts
                else:
                    print(f"Error: Column 'text' not found in {filepath}")
        except Exception as e:
            print(f"Error reading {filepath}: {e}")

# -----------------------------------------------------------------------------
# Main 用于简单的测试，看看能不能读到数据

if __name__ == "__main__":
    print(f"Checking data in: {DATA_DIR}")
    files = list_parquet_files()
    print(f"Found {len(files)} shards.")
    
    if files:
        print("\nTesting 'train' split iterator (reading first batch only):")
        # 测试读取第一个 batch
        iterator = parquets_iter_batched("train")
        try:
            batch = next(iterator)
            print(f"Successfully read a batch of {len(batch)} IPs!")
            print(f"First 5 IPs: {batch[:5]}")
        except StopIteration:
            print("Train iterator is empty!")
            
        print("\nTesting 'val' split iterator:")
        iterator = parquets_iter_batched("val")
        try:
            batch = next(iterator)
            print(f"Successfully read a batch of {len(batch)} IPs from validation shard!")
        except StopIteration:
            print("Val iterator is empty!")


# """
# The base/pretraining dataset is a set of parquet files.
# This file contains utilities for:
# - iterating over the parquet files and yielding documents from it
# - download the files on demand if they are not on disk

# For details of how the dataset was prepared, see `repackage_data_reference.py`.
# """

# import os
# import argparse
# import time
# import requests
# import pyarrow.parquet as pq
# from multiprocessing import Pool

# from nanochat.common import get_base_dir

# # -----------------------------------------------------------------------------
# # The specifics of the current pretraining dataset

# # The URL on the internet where the data is hosted and downloaded from on demand
# BASE_URL = "https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle/resolve/main"
# MAX_SHARD = 1822 # the last datashard is shard_01822.parquet
# index_to_filename = lambda index: f"shard_{index:05d}.parquet" # format of the filenames
# base_dir = get_base_dir()
# DATA_DIR = os.path.join(base_dir, "base_data")
# os.makedirs(DATA_DIR, exist_ok=True)

# # -----------------------------------------------------------------------------
# # These functions are useful utilities to other modules, can/should be imported

# def list_parquet_files(data_dir=None):
#     """ Looks into a data dir and returns full paths to all parquet files. """
#     data_dir = DATA_DIR if data_dir is None else data_dir
#     parquet_files = sorted([
#         f for f in os.listdir(data_dir)
#         if f.endswith('.parquet') and not f.endswith('.tmp')
#     ])
#     parquet_paths = [os.path.join(data_dir, f) for f in parquet_files]
#     return parquet_paths

# def parquets_iter_batched(split, start=0, step=1):
#     """
#     Iterate through the dataset, in batches of underlying row_groups for efficiency.
#     - split can be "train" or "val". the last parquet file will be val.
#     - start/step are useful for skipping rows in DDP. e.g. start=rank, step=world_size
#     """
#     assert split in ["train", "val"], "split must be 'train' or 'val'"
#     parquet_paths = list_parquet_files()
#     parquet_paths = parquet_paths[:-1] if split == "train" else parquet_paths[-1:]
#     for filepath in parquet_paths:
#         pf = pq.ParquetFile(filepath)
#         for rg_idx in range(start, pf.num_row_groups, step):
#             rg = pf.read_row_group(rg_idx)
#             texts = rg.column('text').to_pylist()
#             yield texts

# # -----------------------------------------------------------------------------
# def download_single_file(index):
#     """ Downloads a single file index, with some backoff """

#     # Construct the local filepath for this file and skip if it already exists
#     filename = index_to_filename(index)
#     filepath = os.path.join(DATA_DIR, filename)
#     if os.path.exists(filepath):
#         print(f"Skipping {filepath} (already exists)")
#         return True

#     # Construct the remote URL for this file
#     url = f"{BASE_URL}/{filename}"
#     print(f"Downloading {filename}...")

#     # Download with retries
#     max_attempts = 5
#     for attempt in range(1, max_attempts + 1):
#         try:
#             response = requests.get(url, stream=True, timeout=30)
#             response.raise_for_status()
#             # Write to temporary file first
#             temp_path = filepath + f".tmp"
#             with open(temp_path, 'wb') as f:
#                 for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks
#                     if chunk:
#                         f.write(chunk)
#             # Move temp file to final location
#             os.rename(temp_path, filepath)
#             print(f"Successfully downloaded {filename}")
#             return True

#         except (requests.RequestException, IOError) as e:
#             print(f"Attempt {attempt}/{max_attempts} failed for {filename}: {e}")
#             # Clean up any partial files
#             for path in [filepath + f".tmp", filepath]:
#                 if os.path.exists(path):
#                     try:
#                         os.remove(path)
#                     except:
#                         pass
#             # Try a few times with exponential backoff: 2^attempt seconds
#             if attempt < max_attempts:
#                 wait_time = 2 ** attempt
#                 print(f"Waiting {wait_time} seconds before retry...")
#                 time.sleep(wait_time)
#             else:
#                 print(f"Failed to download {filename} after {max_attempts} attempts")
#                 return False

#     return False


# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(description="Download FineWeb-Edu 100BT dataset shards")
#     parser.add_argument("-n", "--num-files", type=int, default=-1, help="Number of shards to download (default: -1), -1 = disable")
#     parser.add_argument("-w", "--num-workers", type=int, default=4, help="Number of parallel download workers (default: 4)")
#     args = parser.parse_args()

#     num = MAX_SHARD + 1 if args.num_files == -1 else min(args.num_files, MAX_SHARD + 1)
#     ids_to_download = list(range(num))
#     print(f"Downloading {len(ids_to_download)} shards using {args.num_workers} workers...")
#     print(f"Target directory: {DATA_DIR}")
#     print()
#     with Pool(processes=args.num_workers) as pool:
#         results = pool.map(download_single_file, ids_to_download)

#     # Report results
#     successful = sum(1 for success in results if success)
#     print(f"Done! Downloaded: {successful}/{len(ids_to_download)} shards to {DATA_DIR}")

# """
# repackage 脚本 是把你的 TXT 变成 Parquet 的工厂。你只需要运行一次。
# dataset.py 是训练脚本调用的接口。它去读那些 Parquet 文件。
# 关键点：确保 prepare_ipv6_parquet.py 里生成的 Parquet 文件中，列名必须叫 "text"，因为 dataset.py 里的 rg.column('text') 写死了要读这一列。
# """

# # import os
# # import pyarrow.parquet as pq
# # from nanochat.common import get_base_dir

# # # 指向你刚才生成的目录
# # base_dir = get_base_dir()
# # DATA_DIR = os.path.join(base_dir, "base_data")

# # def list_parquet_files(data_dir=None):
# #     """ 列出所有 parquet 文件 """
# #     data_dir = DATA_DIR if data_dir is None else data_dir
# #     if not os.path.exists(data_dir):
# #         return []
# #     parquet_files = sorted([
# #         f for f in os.listdir(data_dir)
# #         if f.endswith('.parquet')
# #     ])
# #     return [os.path.join(data_dir, f) for f in parquet_files]

# # def parquets_iter_batched(split, start=0, step=1):
# #     """
# #     NanoChat 的标准接口。
# #     split="train": 读取除了最后一个之外的所有文件
# #     split="val": 只读取最后一个文件
# #     """
# #     parquet_paths = list_parquet_files()
    
# #     if not parquet_paths:
# #         print(f"Warning: No data found in {DATA_DIR}")
# #         return

# #     # 简单的 train/val 切分逻辑
# #     if split == "train":
# #         files_to_read = parquet_paths[:-1] # 前面所有
# #     else:
# #         files_to_read = parquet_paths[-1:] # 最后一个

# #     for filepath in files_to_read:
# #         try:
# #             pf = pq.ParquetFile(filepath)
# #             # 按 row_group 迭代读取
# #             for rg_idx in range(start, pf.num_row_groups, step):
# #                 rg = pf.read_row_group(rg_idx)
# #                 texts = rg.column('text').to_pylist()
# #                 yield texts # 返回一个 list[str]，即一批 IP 地址
# #         except Exception as e:
# #             print(f"Error reading {filepath}: {e}")

# # # 删除所有的 download_single_file 和 main 函数，因为你不需要自动下载